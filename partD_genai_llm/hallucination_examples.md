**Hallucination 1:** The model generated fake or non-existent research references.  

**Reason:** Large Language Models generate responses by predicting likely text patterns, not by verifying real-world facts.  

**Mitigation:** Use techniques like Retrieval-Augmented Generation (RAG), fact-checking/verification methods, and constrained decoding to reduce hallucinations.  
