- A high learning rate can make training unstable, causing the loss to diverge due to large weight updates.  


- Batch Normalization helps keep gradients stable, leading to smoother and faster training.  


- Dropout may reduce training accuracy, but it improves validation performance by reducing overfitting.  

